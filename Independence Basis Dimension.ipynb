{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank\n",
    "\n",
    "The **rank**, $r$, of a matrix is the number of pivots produced during elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Independence\n",
    "\n",
    "Here are two matrices in $\\mathbf{R^3}$\n",
    "\n",
    "\\begin{align}\n",
    "A &= \n",
    "\\begin{bmatrix}\n",
    "1 && 1 && 1 \\\\\n",
    "1 && 1 && 0 \\\\\n",
    "1 && 0 && 0\n",
    "\\end{bmatrix} &=&\n",
    "\\begin{bmatrix}\n",
    "\\vec{v_1} && \\vec{v_2} && \\vec{v_3}\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "B &= \n",
    "\\begin{bmatrix}\n",
    "1 && 1 \\\\\n",
    "1 && 1 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix} &=&\n",
    "\\begin{bmatrix}\n",
    "\\vec{v_1} && \\vec{v_2}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The vectors $\\vec{v_1}$, $\\vec{v_2}$, and $\\vec{v_3}$ are **linearly independent** when the only solution to $A\\vec{x}=0$ is the zero vector.\n",
    "\n",
    "In this case, all three vectors are linearly independent. We cannot make any of the three vectors using linear combinations of the others. \n",
    "\n",
    "When doing elmination, we get one pivot for every linearly independent column.\n",
    "\n",
    "Geometrically:\n",
    "- Dependent vectors in $\\mathbf{R^3}$ can lie on a plane (or a line, or the nullspace)\n",
    "- Three vectors in $\\mathbf{R^3}$ that are linearly dependent cannot span all of $\\mathbf{R^3}$\n",
    "- Dependent vectors in $\\mathbf{R^2}$ can lie on a line\n",
    "\n",
    "\n",
    "## Full column rank\n",
    "\n",
    "The columns of $A$ are linearly independent exactly when every column has a  pivot $r=n$. This could be a tall skinny matrix.\n",
    "\n",
    "Wide matrices ($n > m$) are gauranteed to have dependent columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanning a Space\n",
    "\n",
    "- A sequence of vectors (such as $\\vec{v_1}$, $\\vec{v_2}$, and $\\vec{v_3}$) **span**  a space.\n",
    "- For every vector sequence, we can describe the space that those vectors span. \n",
    "- That space consists of all linear combinations of the vectors\n",
    "\n",
    "If we have $n$ linearly independent vectors in $\\mathbf{R^n}$, those vectors span $\\mathbf{R^n}$. If we have fewer than $n$ linearly in dependent vectors, those vectors span a subspace of $\\mathbf{R^n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis for a Vector Space\n",
    "\n",
    "A **basis** for a vector space is a sequence of independent vectors that span that space.\n",
    "\n",
    "The vectors $\\vec{v_1} \\ldots \\vec{v_n}$ are a basis for $\\mathbf{R^n}$ exactly when they are the columns of an $n$ by $n$ invertible matrix.\n",
    "\n",
    "- The pivot columns of $A$ are a basis for the column space of $A$\n",
    "- The pivot rows of $A$ are a basis for the row space\n",
    "- The pivot rows of the echelon form, $R$,  are a basis for the row space (p. 173)\n",
    "  - $A$ and $R$ have the same row space, but not the same column space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension of a Vector Space\n",
    "\n",
    "The dimension of a space is the number of vectors needed to span that space.\n",
    "\n",
    "If our subspace in a plane in $\\mathbf{R^3}$, the dimension is $2$ (even though we are in $\\mathbf{R^3}$).\n",
    "\n",
    "The dimension of the column space equals the rank of the matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensions of the Four Subspaces of $A$\n",
    "\n",
    "The row and the column space have the same dimension.\n",
    "\n",
    "- If the column space of $A$ is 3 dimensional, the row space is also 3 dimensional\n",
    "- The dimension of the left nullspace depends on the height of the matrix and the number of pivots\n",
    "\n",
    "Consider this 4 by 3 matrix:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "1 && 1 && 1 \\\\\n",
    "1 && 1 && 0 \\\\\n",
    "1 && 0 && 0 \\\\\n",
    "0 && 0 && 0 \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{align*}\n",
    "&\\text{Rank} = r &&= 3 \\\\\n",
    "&\\text{Dimension of the column space} = r &&= 3\\\\\n",
    "&\\text{Dimension of the row space} = r &&= 3 \\\\\n",
    "&\\text{Dimension of the null space} = n - r = 3 - 3 &&= 0 \\text{ in } \\mathbf{R^m} = \\mathbf{R^4} \\\\\n",
    "&\\text{Dimension of the left null space} = m - r = 4 - 3 &&= 1 \\text{ in } \\mathbf{R^n} = \\mathbf{R^3} \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal Bases\n",
    "\n",
    "Vectors are **orthonormal** exactly when \n",
    "\n",
    "- The length of each vector is $1$, and \n",
    "- Each vector is perpendicular to all other vectors in the sequence: $\\vec{q_i}^T\\vec{q_j} = 0$ when $i \\neq j$\n",
    "\n",
    "A matrix is **orthogonal** when all the column vectors are **orthonormal**.\n",
    "\n",
    "We call an orthogonal matrix $Q$. Repeating the same explaination above we can describe $Q$ as\n",
    "\n",
    "$Q^TQ=I$\n",
    "\n",
    "And when $Q$ is square, $Q^{-1} = Q^T$\n",
    "\n",
    "$Q^TQ=I$  makes it very easy to project or solve for least squares. When the projection matrix is orthogonal:\n",
    "\n",
    "- The least squares solution, $A^TA\\hat{x}=A^T\\vec{b}$ becomes $\\hat{x}=Q^T\\vec{b}$\n",
    "- The projection matrix, $P =  A(A^TA)^{-1}A^T$, becomes $QQ^T$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This example matrix is taken from p. 233 of Gilbert Strang's textbook\n",
    "Q = np.matrix([\n",
    "        [-1,  2,  2],\n",
    "        [ 2, -1,  2], \n",
    "        [ 2,  2, -1]]) * (1/3.)\n",
    "\n",
    "print(Q * Q.transpose())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Orthonormal Vectors\n",
    "\n",
    "Start with three independent vectors, $\\vec{a}, \\vec{b}, \\vec{c}$. Convert these vectors to three orthonormal vectors, $A, B, C$.\n",
    "\n",
    "Leave $\\vec{a}$ as it is:\n",
    "\n",
    "$A = \\vec{a}$\n",
    "\n",
    "Remember how we project a vector onto another vector. To project $\\vec{b}$ onto $A$ (assume $A$ is a vector, not a matrix):\n",
    "\n",
    "$\\vec{p} = \\dfrac{A^T\\vec{b}}{A^TA}A$\n",
    "\n",
    "If we take the part of $\\vec{b}$ that is in the direction of $A$, and subtract it from $\\vec{b}$, we will be left with the part of $\\vec{b}$ that is orthogonal to $A$.\n",
    "\n",
    "$B = \\vec{b} - \\dfrac{A^T\\vec{b}}{A^TA}A$\n",
    "\n",
    "Finally, subtract from $\\vec{c}$ the parts of $\\vec{c}$ that are orthogonal to $A$ and $B$.\n",
    "\n",
    "$C = \\vec{c} - \\dfrac{A^T\\vec{c}}{A^TA}A - \\dfrac{B^T\\vec{c}}{B^TB}B$\n",
    "\n",
    "Now we have three orthogonal vectors. To make them orthonormal, divide each vector by its length:\n",
    "\n",
    "\\begin{align*}\n",
    "q_1 = \\tfrac{A}{\\|A\\|}\\\\\n",
    "q_2 = \\tfrac{B}{\\|B\\|}\\\\\n",
    "q_3 = \\tfrac{C}{\\|C\\|}\n",
    "\\end{align*}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
